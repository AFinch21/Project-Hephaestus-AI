{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM: 9.50/15.16GB\t RAM:[▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▯▯▯▯▯▯▯▯]\n",
      "VRAM: 5.29/11.99GB\t VRAM:[▮▮▮▮▮▮▮▮▮▮▮▯▯▯▯▯▯▯▯▯▯▯▯▯]\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import torch\n",
    "\n",
    "def get_ram():\n",
    "    mem = psutil.virtual_memory()\n",
    "    free = mem.available / 1024 ** 3\n",
    "    total = mem.total / 1024 ** 3\n",
    "    total_cubes = 24\n",
    "    free_cubes = int(total_cubes * free / total)\n",
    "    return f'RAM: {total - free:.2f}/{total:.2f}GB\\t RAM:[' + (total_cubes - free_cubes) * '▮' + free_cubes * '▯' + ']'\n",
    "\n",
    "\n",
    "def get_vram():\n",
    "    free = torch.cuda.mem_get_info()[0] / 1024 ** 3\n",
    "    total = torch.cuda.mem_get_info()[1] / 1024 ** 3\n",
    "    total_cubes = 24\n",
    "    free_cubes = int(total_cubes * free / total)\n",
    "    return f'VRAM: {total - free:.2f}/{total:.2f}GB\\t VRAM:[' + (\n",
    "            total_cubes - free_cubes) * '▮' + free_cubes * '▯' + ']'\n",
    "    \n",
    "print(get_ram())\n",
    "print(get_vram())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\Project-Hephaestus-AI\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from Utilities.ModelInference import ModelInference\n",
    "\n",
    "codellama = ModelInference(\"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif true == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\'t know the answer to a question, please don\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\n' + content.strip() + '\\n<</SYS>>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "<s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST]\n"
     ]
    }
   ],
   "source": [
    "print(codellama.tokenizer.default_chat_template)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "    \n",
    "print(codellama.tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST]\n",
      "<s><s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] Certainly! Here's a simple recipe for homemade mayonnaise that you can try:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 1 large egg yolk\n",
      "* 1 tablespoon Dijon mustard\n",
      "* 1 tablespoon white wine vinegar or lemon juice\n",
      "* 1/2 cup vegetable oil\n",
      "* Salt and pepper to taste\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. In a small bowl, whisk together the egg yolk, mustard, and vinegar or lemon juice until well combined.\n",
      "2. Slowly drizzle in the vegetable oil while whisking constantly to ensure that the oil is emulsified into the egg mixture.\n",
      "3. Once the oil has been added, season the mayonnaise with salt and pepper to taste.\n",
      "4. Serve immediately or store in an airtight container in the refrigerator for up to 3 days.\n",
      "\n",
      "I hope you enjoy this recipe! Let me know if you have any other questions.</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s><s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] Certainly! Here's a simple recipe for homemade mayonnaise that you can try:\\n\\nIngredients:\\n\\n* 1 large egg yolk\\n* 1 tablespoon Dijon mustard\\n* 1 tablespoon white wine vinegar or lemon juice\\n* 1/2 cup vegetable oil\\n* Salt and pepper to taste\\n\\nInstructions:\\n\\n1. In a small bowl, whisk together the egg yolk, mustard, and vinegar or lemon juice until well combined.\\n2. Slowly drizzle in the vegetable oil while whisking constantly to ensure that the oil is emulsified into the egg mixture.\\n3. Once the oil has been added, season the mayonnaise with salt and pepper to taste.\\n4. Serve immediately or store in an airtight container in the refrigerator for up to 3 days.\\n\\nI hope you enjoy this recipe! Let me know if you have any other questions.</s>\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "codellama.infer(messages, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\Project-Hephaestus-AI\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''<s>[INST] {prompt} [/INST]\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif true == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\'t know the answer to a question, please don\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\n' + content.strip() + '\\n<</SYS>>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "<s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.default_chat_template)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "    \n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s>[INST] Tell me about AI [/INST]\n",
      "AI stands for Artificial Intelligence, which is a branch of computer science that focuses on creating intelligent machines that can perform tasks that would normally require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.\n",
      "\n",
      "AI systems can be categorized into two main types: narrow or weak AI, and general or strong AI. Narrow AI is designed to perform a specific task, such as recognizing faces or playing chess, while general AI has the ability to learn and perform a wide range of tasks.\n",
      "\n",
      "AI is used in many industries, including healthcare, finance, transportation, and manufacturing. It has the potential to revolutionize the way we live and work by automating tasks, improving efficiency, and enabling new forms of communication and collaboration.\n",
      "\n",
      "However, there are also concerns about the ethical and societal implications of AI, such as the potential for bias, privacy violations, and job displacement. As AI technology continues to advance, it will be important to carefully consider these issues and ensure that AI is developed and used in a responsible and ethical manner.</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralConfig {\n",
       "  \"_name_or_path\": \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\",\n",
       "  \"architectures\": [\n",
       "    \"MistralForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"model_type\": \"mistral\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"quantization_config\": {\n",
       "    \"batch_size\": 1,\n",
       "    \"bits\": 4,\n",
       "    \"block_name_to_quantize\": null,\n",
       "    \"cache_block_outputs\": true,\n",
       "    \"damp_percent\": 0.1,\n",
       "    \"dataset\": null,\n",
       "    \"desc_act\": true,\n",
       "    \"exllama_config\": {\n",
       "      \"version\": 1\n",
       "    },\n",
       "    \"group_size\": 128,\n",
       "    \"max_input_length\": null,\n",
       "    \"model_seqlen\": null,\n",
       "    \"module_name_preceding_first_block\": null,\n",
       "    \"modules_in_block_to_quantize\": null,\n",
       "    \"pad_token_id\": null,\n",
       "    \"quant_method\": \"gptq\",\n",
       "    \"sym\": true,\n",
       "    \"tokenizer\": null,\n",
       "    \"true_sequential\": true,\n",
       "    \"use_cuda_fp16\": false,\n",
       "    \"use_exllama\": true\n",
       "  },\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": 4096,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.43.0.dev0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Model': 'No Model Loaded',\n",
       " 'vram_total': 11.99365234375,\n",
       " 'vram_used': 1.14599609375,\n",
       " 'vram_free': 10.84765625,\n",
       " 'ram_total': 15.156818389892578,\n",
       " 'ram_used': 10.435836791992188,\n",
       " 'ram_free': 4.720981597900391}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def get_model_stats():\n",
    "\n",
    "    # # API endpoint\n",
    "    url = 'http://localhost:8081/get_model_stats/'\n",
    "    \n",
    "    # Making the POST request\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Checking if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        response = json.loads(response.text)\n",
    "        return response\n",
    "    else:\n",
    "        print('Bad response')\n",
    "        return \n",
    "    \n",
    "stats = get_model_stats()\n",
    "stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
