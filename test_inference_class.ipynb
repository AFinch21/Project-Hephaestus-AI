{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM: 12.38/15.16GB\t RAM:[▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▯▯▯▯]\n",
      "VRAM: 1.15/11.99GB\t VRAM:[▮▮▮▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯]\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import torch\n",
    "\n",
    "def get_ram():\n",
    "    mem = psutil.virtual_memory()\n",
    "    free = mem.available / 1024 ** 3\n",
    "    total = mem.total / 1024 ** 3\n",
    "    total_cubes = 24\n",
    "    free_cubes = int(total_cubes * free / total)\n",
    "    return f'RAM: {total - free:.2f}/{total:.2f}GB\\t RAM:[' + (total_cubes - free_cubes) * '▮' + free_cubes * '▯' + ']'\n",
    "\n",
    "\n",
    "def get_vram():\n",
    "    free = torch.cuda.mem_get_info()[0] / 1024 ** 3\n",
    "    total = torch.cuda.mem_get_info()[1] / 1024 ** 3\n",
    "    total_cubes = 24\n",
    "    free_cubes = int(total_cubes * free / total)\n",
    "    return f'VRAM: {total - free:.2f}/{total:.2f}GB\\t VRAM:[' + (\n",
    "            total_cubes - free_cubes) * '▮' + free_cubes * '▯' + ']'\n",
    "    \n",
    "print(get_ram())\n",
    "print(get_vram())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\Project-Hephaestus-AI\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from Utilities.ModelInference import ModelInference\n",
    "\n",
    "codellama = ModelInference(\"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif true == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\'t know the answer to a question, please don\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\n' + content.strip() + '\\n<</SYS>>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "<s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST]\n"
     ]
    }
   ],
   "source": [
    "print(codellama.tokenizer.default_chat_template)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "    \n",
    "print(codellama.tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST]\n",
      "<s><s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] Certainly! Here's a simple recipe for homemade mayonnaise that you can try:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 1 large egg yolk\n",
      "* 1 tablespoon Dijon mustard\n",
      "* 1 tablespoon white wine vinegar or lemon juice\n",
      "* 1/2 cup vegetable oil\n",
      "* Salt and pepper to taste\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. In a small bowl, whisk together the egg yolk, mustard, and vinegar or lemon juice until well combined.\n",
      "2. Slowly drizzle in the vegetable oil while whisking constantly to ensure that the oil is emulsified into the egg mixture.\n",
      "3. Once the oil has been added, season the mayonnaise with salt and pepper to taste.\n",
      "4. Serve immediately or store in an airtight container in the refrigerator for up to 3 days.\n",
      "\n",
      "I hope you enjoy this recipe! Let me know if you have any other questions.</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s><s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] Certainly! Here's a simple recipe for homemade mayonnaise that you can try:\\n\\nIngredients:\\n\\n* 1 large egg yolk\\n* 1 tablespoon Dijon mustard\\n* 1 tablespoon white wine vinegar or lemon juice\\n* 1/2 cup vegetable oil\\n* Salt and pepper to taste\\n\\nInstructions:\\n\\n1. In a small bowl, whisk together the egg yolk, mustard, and vinegar or lemon juice until well combined.\\n2. Slowly drizzle in the vegetable oil while whisking constantly to ensure that the oil is emulsified into the egg mixture.\\n3. Once the oil has been added, season the mayonnaise with salt and pepper to taste.\\n4. Serve immediately or store in an airtight container in the refrigerator for up to 3 days.\\n\\nI hope you enjoy this recipe! Let me know if you have any other questions.</s>\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "codellama.infer(messages, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''<s>[INST] {prompt} [/INST]\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif true == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\'t know the answer to a question, please don\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\n' + content.strip() + '\\n<</SYS>>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "<s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.default_chat_template)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "    \n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s>[INST] Tell me about AI [/INST]\n",
      "\n",
      "AI (Artificial Intelligence) refers to the development of computer systems that can perform tasks that would typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI technologies are designed to enable machines to learn, reason, and make decisions in a way that is similar to how humans do.\n",
      "\n",
      "AI can be categorized into three main categories: narrow or weak AI, general or strong AI, and superintelligent AI. Narrow AI is designed for a specific task, such as image recognition or language translation, and is limited in its ability to perform other tasks. General AI is designed to be capable of performing a wide range of tasks and is more similar to human intelligence. Superintelligent AI is designed to be capable of surpassing human intelligence in all areas.\n",
      "\n",
      "AI is being developed and used in a variety of fields, including healthcare, finance, transportation, and education. It has the potential to revolutionize the way we live and work, and to solve some of the world's most pressing problems. However, it also raises important ethical and social questions, such as how to ensure that AI is developed and used in a responsible and ethical manner.</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
