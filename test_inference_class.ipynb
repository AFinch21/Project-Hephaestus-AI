{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM: 10.45/15.16GB\t RAM:[▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▯▯▯▯▯▯▯]\n",
      "VRAM: 1.15/11.99GB\t VRAM:[▮▮▮▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯▯]\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import torch\n",
    "\n",
    "def get_ram():\n",
    "    mem = psutil.virtual_memory()\n",
    "    free = mem.available / 1024 ** 3\n",
    "    total = mem.total / 1024 ** 3\n",
    "    total_cubes = 24\n",
    "    free_cubes = int(total_cubes * free / total)\n",
    "    return f'RAM: {total - free:.2f}/{total:.2f}GB\\t RAM:[' + (total_cubes - free_cubes) * '▮' + free_cubes * '▯' + ']'\n",
    "\n",
    "\n",
    "def get_vram():\n",
    "    free = torch.cuda.mem_get_info()[0] / 1024 ** 3\n",
    "    total = torch.cuda.mem_get_info()[1] / 1024 ** 3\n",
    "    total_cubes = 24\n",
    "    free_cubes = int(total_cubes * free / total)\n",
    "    return f'VRAM: {total - free:.2f}/{total:.2f}GB\\t VRAM:[' + (\n",
    "            total_cubes - free_cubes) * '▮' + free_cubes * '▯' + ']'\n",
    "    \n",
    "print(get_ram())\n",
    "print(get_vram())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\Project-Hephaestus\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from Utilities.ModelInference import ModelInference\n",
    "\n",
    "codellama = ModelInference(\"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif true == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\'t know the answer to a question, please don\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\n' + content.strip() + '\\n<</SYS>>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "<s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST]\n"
     ]
    }
   ],
   "source": [
    "print(codellama.tokenizer.default_chat_template)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "    \n",
    "print(codellama.tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST]\n",
      "<s><s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] Certainly! Mayonnaise is a classic condiment that's easy to make at home. Here's a simple recipe:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 2 large egg yolks\n",
      "* 1 tablespoon Dijon mustard\n",
      "* 2 tablespoons white wine vinegar\n",
      "* 1 tablespoon lemon juice\n",
      "* 1/2 teaspoon salt\n",
      "* 1/4 teaspoon black pepper\n",
      "* 1/2 cup vegetable oil\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. In a blender or food processor, combine the egg yolks, mustard, vinegar, lemon juice, salt, and pepper.\n",
      "2. With the blender or food processor running, slowly pour in the vegetable oil in a steady stream.\n",
      "3. Blend until the mixture is smooth and creamy.\n",
      "4. Taste the mayonnaise and adjust the seasoning as desired.\n",
      "5. Transfer the mayonnaise to an airtight container and refrigerate until ready to serve.\n",
      "\n",
      "That's it! You can use this homemade mayonnaise in all your favourite recipes. Enjoy!</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s><s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] Certainly! Mayonnaise is a classic condiment that's easy to make at home. Here's a simple recipe:\\n\\nIngredients:\\n\\n* 2 large egg yolks\\n* 1 tablespoon Dijon mustard\\n* 2 tablespoons white wine vinegar\\n* 1 tablespoon lemon juice\\n* 1/2 teaspoon salt\\n* 1/4 teaspoon black pepper\\n* 1/2 cup vegetable oil\\n\\nInstructions:\\n\\n1. In a blender or food processor, combine the egg yolks, mustard, vinegar, lemon juice, salt, and pepper.\\n2. With the blender or food processor running, slowly pour in the vegetable oil in a steady stream.\\n3. Blend until the mixture is smooth and creamy.\\n4. Taste the mayonnaise and adjust the seasoning as desired.\\n5. Transfer the mayonnaise to an airtight container and refrigerate until ready to serve.\\n\\nThat's it! You can use this homemade mayonnaise in all your favourite recipes. Enjoy!</s>\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "codellama.infer(messages, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\local_gpt_testing\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''<s>[INST] {prompt} [/INST]\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif true == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\'t know the answer to a question, please don\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\n' + content.strip() + '\\n<</SYS>>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "<s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.default_chat_template)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "    \n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s>[INST] Tell me about AI [/INST]\n",
      "AI stands for Artificial Intelligence. It is a field of computer science that aims to create intelligent machines that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI systems use algorithms and machine learning techniques to analyze data and make predictions or decisions based on that data. They can also learn from new data and improve their performance over time.\n",
      "\n",
      "There are many different types of AI, including narrow or weak AI, which is designed to perform a specific task, and general or strong AI, which is designed to be capable of performing any intellectual task that a human can. AI has the potential to revolutionize many industries, including healthcare, finance, transportation, and education. It is already being used in a variety of applications, such as virtual assistants, self-driving cars, and medical diagnosis.</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
